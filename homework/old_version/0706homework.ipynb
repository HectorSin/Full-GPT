{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac1b5bd3",
   "metadata": {},
   "source": [
    "Stuff Documents 체인을 사용하여 완전한 RAG 파이프라인을 구현하세요.  \n",
    "체인을 수동으로 구현해야 합니다.  \n",
    "체인에 ConversationBufferMemory를 부여합니다.  \n",
    "이 문서를 사용하여 RAG를 수행하세요: https://gist.github.com/serranoarevalo/5acf755c2b8d83f1707ef266b82ea223  \n",
    "체인에 다음 질문을 합니다:  \n",
    "Aaronson 은 유죄인가요?  \n",
    "그가 테이블에 어떤 메시지를 썼나요?  \n",
    "Julia 는 누구인가요?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83453609",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f874a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=120,\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "def load_memory(_):\n",
    "    return memory.load_memory_variables({})[\"history\"]\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"검색된 문서들을 문자열로 포맷팅\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "\n",
    "loader = UnstructuredFileLoader(\"./files/document.txt\")\n",
    "\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "stuff_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant. Answer questions using only the following context. If you don't know the answer just say you don't know, don't make it up:\\n\\n{context}\n",
    "            {context}\n",
    "            \"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "stuff_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.get_relevant_documents(x[\"question\"])),\n",
    "        history=load_memory\n",
    "    )\n",
    "    | stuff_prompt \n",
    "    | llm\n",
    ")\n",
    "\n",
    "def invoke_chain(question):\n",
    "    result = stuff_chain.invoke({\"question\": question})\n",
    "    memory.save_context({\"input\": question}, {\"output\": result.content})\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd6c5b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='제가 알기로는 Aaronson이 유죄인지 아닌지에 대한 정보는 없습니다.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Aaronson 은 유죄인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8fee28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='그가 테이블에 쓴 메시지는 다음과 같습니다:\\nFREEDOM IS SLAVERY\\nTWO AND TWO MAKE FIVE\\nGOD IS POWER'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"그가 테이블에 어떤 메시지를 썼나요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0813dcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Julia는 Winston과 함께 이야기하는 여성 캐릭터입니다. 그들은 사랑을 나누었지만, 파티의 강압에 맞서 싸우는 동안 서로를 배신하게 되었습니다.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"Julia 는 누구인가요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bafb33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='The human asks if Aaronson is guilty. The AI states that it does not have information on whether Aaronson is guilty or not. The human asks what message Aaronson wrote on the table, and the AI reveals the messages: \"FREEDOM IS SLAVERY\", \"TWO AND TWO MAKE FIVE\", \"GOD IS POWER\".'),\n",
       "  HumanMessage(content='Julia 는 누구인가요?'),\n",
       "  AIMessage(content='Julia는 Winston과 함께 이야기하는 여성 캐릭터입니다. 그들은 사랑을 나누었지만, 파티의 강압에 맞서 싸우는 동안 서로를 배신하게 되었습니다.')]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3109e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='첫 번째 질문은 \"Aaronson이 유죄인가요?\" 였습니다.'\n"
     ]
    }
   ],
   "source": [
    "invoke_chain(\"첫번째 질문이 뭐였나요?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "full-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
